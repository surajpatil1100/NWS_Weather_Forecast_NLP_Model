import requests

URL = 'https://api.weather.gov/zones/forecast/MNZ060/forecast'

response = requests.get(URL)

forecast = response.json()['properties']

print('Forecast updated:', forecast['updated'])
print('')

for period in forecast['periods']:
    print(period['name'])
    print(period['detailedForecast'])
    print('')

import pandas as pd
forecast_data=[]
for period in forecast['periods']:
   forecast_data.append({ 'name': period['name'],
                         'detailedForecast': period['detailedForecast'] })
df=pd.DataFrame(forecast_data)
print(df)

# prompt: perform nlp operations on detailedForecast column

import requests
import pandas as pd
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Download required NLTK data (only needs to be done once)
try:
    nltk.data.find('vader_lexicon')
except LookupError:
    nltk.download('vader_lexicon')


URL = 'https://api.weather.gov/zones/forecast/MNZ060/forecast'

response = requests.get(URL)

forecast = response.json()['properties']

#print('Forecast updated:', forecast['updated'])
#print('')

forecast_data=[]
for period in forecast['periods']:
   forecast_data.append({ 'name': period['name'],
                         'detailedForecast': period['detailedForecast'] })
df=pd.DataFrame(forecast_data)
#print(df)

# Initialize VADER sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

# Function to get sentiment scores
def get_sentiment(text):
    scores = analyzer.polarity_scores(text)
    return scores['compound']


# Apply sentiment analysis to the 'detailedForecast' column
df['sentiment_score'] = df['detailedForecast'].apply(get_sentiment)

# Print the DataFrame with sentiment scores
print(df)

# Example: Filter for positive forecasts
positive_forecasts = df[df['sentiment_score'] > 0.5]
print("\nPositive Forecasts:")
positive_forecasts

forecast_string = ' '.join(df['detailedForecast'].tolist())
print(forecast_string)

import nltk
nltk.download('punkt_tab')

import re
import nltk
from nltk.tokenize import word_tokenize
from nltk import FreqDist

# Download necessary NLTK data files
nltk.download('punkt')

def clean_text(text):
    text = re.sub(r'\n+', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

cleaned_text = clean_text(forecast_string)
tokens = word_tokenize(cleaned_text)

def summarize_text(text):
    words = word_tokenize(text)
    fdist = FreqDist(words)
    summary = ' '.join([word for word, freq in fdist.most_common(50)])
    return summary

summary = summarize_text(cleaned_text)
print("Summary:", summary)

import spacy

nlp = spacy.load('en_core_web_sm')

# Generate extractive summary using spaCy
def spacy_summarize(text):
    doc = nlp(text)
    sentences = [sent.text for sent in doc.sents]
    summary = " ".join(sentences[:6])  # Extract the first 6 sentences as a summary
    return summary

spacy_summary = spacy_summarize(cleaned_text)
print("spaCy Summary:", spacy_summary)

import json 

summary = spacy_summary
positive_forecast = positive_forecasts 

# Save the variables to a JSON file

data = { "summary": summary, "positive_forecast": positive_forecast }
with open('forecast_data.json', 'w') as outfile:
  json.dump(data, outfile)
print("Data saved to forecast_data.json")
